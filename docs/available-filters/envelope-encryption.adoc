:kms-api-javadoc: https://javadoc.io/doc/io.kroxylicious/kroxylicious-kms/latest
:encryption-api-javadoc: https://javadoc.io/doc/io.kroxylicious/kroxylicious-encryption
:design-doc: https://github.com/kroxylicious/kroxylicious/blob/main/kroxylicious-filters/kroxylicious-encryption/doc/design.adoc

= Envelope Encryption

== What is it?

A filter that provides a transparent https://kroxylicious.io/use-cases/[encryption-at-rest solution] for Apache Kafka.

It works as follows:

1. The filter intercepts the requests sent from the producing Kafka Client containing records.  The records
   contained with the request are encrypted.  This happens before the records reach the Kafka Broker.
2. The filter intercepts the responses sent from the Kafka Broker to the consuming Kafka Client.  The records
   contained within the response are decrypted.  This happens before the records reach Kafka Client.

The entire process is transparent from the point of view of both Kafka Client and Kafka Broker.  Neither are
aware that the records are being encrypted.  Neither client nor broker have any access to the encryption keys.

The filter encrypts the kafka records using symmetric encryption keys.  The encryption technique employed is
known as *Envelope Encryption*, which is technique suited for encrypting large volumes of data efficiently.
Envelope Encryption is described in the next section.

The filter integrates with a Key Management Service (KMS). The user must bring a compatible Key Management Service
(KMS) in order to use this solution.  This is talked about later.

=== Envelope Encryption

Envelope Encryption is a technique described by https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf[NIST
Special Publication 800-57 Part 1 Revision 5]. It is the practice of encrypting the data with a data encryption key (DEK),
then wrapping (encrypting) the DEK with a key encryption key (KEK).

KEKs are stored centrally within the Key Management Service (KMS).  The key material of the KEK never leaves the confines
of the KMS.

On produce request, the filter uses the KMS to create a DEK.  The DEK is used to encrypt the record.  The original record
in the produce request is replaced by a *cipher record* which comprises the encrypted record, the encrypted DEK and some other
data (fully described in the {design-doc}[design]).  It is this cipher text record that the Kafka Broker appends to its
transaction log.

On fetch response, the filter receives the *cipher record* from the Kafka Broker. The filter reverses process used to
construct the *cipher record* and uses the KMS to decrypt DEK.  The decrypted DEK is used to decrypt the encrypted record.
The cipher record in the fetch response is then replaced with the decrypted record.  The Kafka Client receives the decrypted record.

The filter employs a DEK reuse strategy and caching to avoid excessive calls to the KMS, whilst ensuring DEKs
are rotated sufficiently often to prevent key exhaustion.

== Prerequisites

In order to use the filter, a compatible KMS (Key Management Service) must be provided.  The KMS is used to store
the KEKs used to encrypt the records sent to the topic.  The service is used at runtime. If the KMS is
unavailable, it will become impossible to produce or consume records through the filter until the KMS service is
restored.  It is recommended to use a KMS in a highly available configuration.

Currently, there is a single KMS integration: https://www.hashicorp.com/[HashiCorp Vault&#174;].  More KMS integrations
are planned.

It is also possible for a user to implement their own KMS integration.  The implementation must implement the
{kms-api-javadoc}/io/kroxylicious/kms/service/KmsService.html[KMS public API] and make the implementation available on
the classpath with the service loader mechanism.

== How to use the filter

There are three steps to using the filter.

1. Setting up the KMS.
2. Configuring the filter within Kroxylicious.
3. Establishing the encryption key(s) within the KMS that will be used to encrypt the topics.

These steps are described in the next sections.

=== Setting up the KMS

In order to set up the KMS provider ready to use with the filter, follow these KMS provider specific steps.

==== HashiCorp Vault

==== Enable the transit engine

The filter integrates with the https://developer.hashicorp.com/vault/docs/secrets/transit[HashiCorp Vault *transit
engine*].   Vault does not enable the transit engine by default.  It must be
https://developer.hashicorp.com/vault/docs/secrets/transit#setup[enabled] before it can be used with the filter.

The transit engine's path must be `/transit` (the default).

==== Generate a Vault token for filter

The filter requires https://developer.hashicorp.com/vault/docs/concepts/tokens[Vault Token] configured with
policy allowing the https://developer.hashicorp.com/vault/api-docs/secret/transit#read-key[read-key],
https://developer.hashicorp.com/vault/api-docs/secret/transit#generate-data-key[generate-data-key], and
https://developer.hashicorp.com/vault/api-docs/secret/transit#decrypt-data[decrypt-data] operations.

It is recommended that dedicated Vault token is used for this purpose. That is, the token is not shared by
another application or human user.

A minimal Vault policy can be established like this:

[source,shell]
----
vault policy write kroxylicious_encryption_policy - << EOF
path "transit/keys/*" {
capabilities = ["read"]
}
path "/transit/datakey/plaintext/*" {
capabilities = ["update"]
}
path "transit/decrypt/*" {
capabilities = [ "update"]
}
EOF
----

A suitable Vault token can be created like this:

[source,shell]
----
vault token create -display-name "kroxylicious encryption"  -no-default-policy -policy=kroxylicious_encryption_policy
----

The `token create` command yields the `token`. The `token` value is required later when configuring the vault within the
filter.

[source]
----
token              fdb90d58-af87-024f-fdcd-9f95039e353a
token_accessor     4cd9177c-034b-a004-c62d-54bc56c0e9bd
token_policies     [kroxylicious_encryption_policy]
----

==== Vault Service URL

Finally, the Vault Service URL is required so the filter knows how to connect to Vault.
This address is reported by Vault as the `Api Address` as it
https://developer.hashicorp.com/vault/tutorials/getting-started/getting-started-dev-server#starting-the-dev-server[starts up].

=== Filter Configuration

The filter is configured as part of the filter chain in the following way:

[source, yaml]
----
filters:
- type: EnvelopeEncryption                                        # <1>
  config:
    kms: <kms service name>                                       # <2>
    kmsConfig:                                                    # <3>
      ..:
    selector: <KEK selector service name>                         # <4>
    selectorConfig:                                               # <5>
      ..:
----
<1> The name of the filter. This must be `EnvelopeEncryption`.
<2> The KMS service name.
<3> Object providing configuration understood by KMS provider.
<4> The KEK selector service name.
<5> Object providing configuration understood by key selector.

==== KMS configuration

For the KMS configuration:

===== HashiCorp Vault

For HashiCorp Vault, the KMS configuration looks like this.  Use the Vault Token and Service URLs values that
you gathered above.

[source, yaml]
----
kms: VaultKmsService                                          # <1>
kmsConfig:
  vaultUrl: <vault service url>                               # <2>
  tls:                                                        # <3>
  vaultToken: <vault token>                                   # <4>
----
<1> Name of the KMS provider. This must be `VaultKmsService`.
<2> Vault URL including the protocol part, i.e. `https:` or `http:`
<3> (Optional) TLS trust configuration.
<4> Vault Token

For TLS trust configuration, the filter accepts the same trust parameters as link:../deploying.adoc#_upstream_tls[Upstream TLS]
except the `PEM` store type is currently https://github.com/kroxylicious/kroxylicious/issues/933[not supported].

==== KEK selector configuration

The role of the KEK selector is to map from the topic name to key name.  The filter looks up the resulting
key name in the KMS.

NOTE: If the filter is unable to find the key in the KMS, the filter will pass through the
records belonging to that topic in the produce request without encrypting them.

===== Template KEK Selector

The `TemplateKekSelector` maps from topic name to key name.  The template understands the substitution token
`$\{topicName}` which is replaced by the name of the topic.  It can be used to build key names
that include the topic name being encrypted.

Use the `$\{topicName}` is optional. It is possible to pass a literal string.  This will result in all topics being
encrypted using the same key.

[source, yaml]
----
selector: TemplateKekSelector                                 # <1>
selectorConfig:
  template: "key_${topicName}"                                # <2>
----
<1> The name of the KEK selector. This must be `TemplateKekSelector`.
<2> Template used to build the key name from the topic name.

=== Establishing the keys in the KMS

Use the management interface of the KMS to create the KEKs. The names (or aliases) of the encryption keys
must match the naming conventions established within the configuration of the KEK selector.  If the selector generates
a key name that doesn't exist within the KMS, records will be sent to the topic without encryption.

For example, if using the `TemplateKekSelector` with the template `kafka_$\{topicName}`, create a key for every topic that
is to be encrypted with the key name matching the topic name, prefixed by the string `kafka_`.

==== HashiCorp Vault

Use either the HashiCorp UI or CLI to create AES-256 symmetric keys following your key naming convention. The key type
must be `aes256-gcm96`, which is Vault's default key type.

TIP: It is recommended to use a key rotation policy.

If using the Vault CLI, the command will look like:

[source, shell]
----
vault write -f transit/keys/kafka_trades type=aes256-gcm96 auto_rotate_period=90d
----

=== Verifying that encryption is occurring

To verify that records sent to topics are indeed being encrypted, use `kafka-console-consumer` to consume the
records *directly from the target Kafka Cluster*.  Verify that encrypted text is seen rather than whatever plain text
that was sent by producer.

[source]
----
kafka-console-consumer --bootstrap-server mycluster:8092 --topic trades --from-beginning
----

The record values seen will look something like this:

[source]
----
tradesvault:v1:+EfJ977UG1XkjI9yh7vxpgN2E1DKaIkDuxE+eCprVTKr+sskFuChcTe/KpR/c8ZDyP76W3itExmEzLOl����x)�Ũ�z�:S�������tБ��v���
----





